from fastapi import APIRouter, Depends, HTTPException
from models.api import OpenAIRequest, OpenAIResponse
from services.retriever import QdrantRetriever
from services.embedder import QueryEmbedder
from services.llm_service import LLMService
from config.settings import Settings
import logging
import time

# ğŸ”§ Configurar logger
logger = logging.getLogger(__name__)

router = APIRouter(prefix="/llm", tags=["llm"])


def get_settings():
    return Settings()


def get_embedder(settings: Settings = Depends(get_settings)):
    return QueryEmbedder(
        dense_model_name=settings.dense_model_name,
        bm25_model_name=settings.bm25_model_name,
        late_interaction_model_name=settings.late_interaction_model_name,
    )


def get_retriever(settings: Settings = Depends(get_settings)):
    return QdrantRetriever(settings=settings)


def get_openai_service(settings: Settings = Depends(get_settings)):
    return LLMService(settings=settings)


@router.post("", response_model=OpenAIResponse)
async def generate_openai_response(
    request: OpenAIRequest,
    embedder: QueryEmbedder = Depends(get_embedder),
    retriever: QdrantRetriever = Depends(get_retriever),
    openai_service: LLMService = Depends(get_openai_service),
):
    start_time = time.time()
    
    try:
        # ğŸ”§ LOG INICIAL
        logger.info(f"ğŸš€ Iniciando LLM para coleÃ§Ã£o: {request.collection_name}")
        logger.info(f"ğŸ“ Query: {request.query[:100]}...")
        logger.info(f"ğŸ”¢ Limit: {request.limit}")

        # 1. GERAR EMBEDDINGS
        logger.info("ğŸ”§ Gerando embeddings da query...")
        query_embeddings = embedder.embed_query(request.query)
        logger.info("âœ… Embeddings gerados com sucesso")

        # 2. BUSCAR NO QDRANT
        logger.info(f"ğŸ” Buscando documentos no Qdrant...")
        context_documents = retriever.search_documents(
            collection_name=request.collection_name, 
            embeddings=query_embeddings, 
            limit=request.limit
        )
        logger.info(f"âœ… {len(context_documents)} documentos encontrados")

        # 3. GERAR RESPOSTA COM OPENAI
        logger.info("ğŸ¤– Chamando OpenAI...")
        answer = openai_service.generate_response(
            query=request.query,
            context_documents=context_documents,
            model=request.model,
            temperature=request.temperature,
            max_output_tokens=request.max_output_tokens,
        )
        logger.info("âœ… Resposta OpenAI gerada com sucesso")

        # ğŸ”§ LOG FINAL
        tempo_total = time.time() - start_time
        logger.info(f"â° LLM concluÃ­do em {tempo_total:.2f} segundos")

        return OpenAIResponse(answer=answer, source_documents=context_documents)

    except HTTPException:
        # Re-lanÃ§a exceÃ§Ãµes HTTP que jÃ¡ sabemos
        raise
        
    except Exception as e:
        # ğŸ”§ LOG DE ERRO DETALHADO
        tempo_total = time.time() - start_time
        logger.error(f"ğŸ’¥ ERRO NO ENDPOINT LLM:")
        logger.error(f"   â° Tempo atÃ© erro: {tempo_total:.2f}s")
        logger.error(f"   ğŸ“ Query: {request.query}")
        logger.error(f"   ğŸ—‚ï¸ ColeÃ§Ã£o: {request.collection_name}")
        logger.error(f"   ğŸš¨ Erro: {str(e)}")
        logger.error(f"   ğŸ“‹ Tipo: {type(e).__name__}")
        
        import traceback
        logger.error(f"   ğŸ” Stack trace: {traceback.format_exc()}")
        
        raise HTTPException(
            status_code=500, 
            detail=f"Erro no processamento LLM: {str(e)}"
        )


# ğŸ”§ ENDPOINT DE TESTE SIMPLES
@router.post("/test")
async def test_llm_endpoint():
    """Endpoint simples para testar se o LLM estÃ¡ funcionando"""
    try:
        logger.info("ğŸ§ª Testando endpoint LLM...")
        
        # Teste mÃ­nimo
        settings = get_settings()
        embedder = get_embedder(settings)
        
        # Apenas gera embeddings de um texto simples
        test_embedding = embedder.embed_query("Teste de funcionamento")
        logger.info("âœ… Teste LLM concluÃ­do com sucesso")
        
        return {
            "status": "success",
            "message": "LLM endpoint estÃ¡ funcionando",
            "embedding_size": len(test_embedding.dense) if hasattr(test_embedding, 'dense') else "N/A"
        }
        
    except Exception as e:
        logger.error(f"ğŸ’¥ Teste LLM falhou: {str(e)}")
        raise HTTPException(500, f"Teste falhou: {str(e)}")